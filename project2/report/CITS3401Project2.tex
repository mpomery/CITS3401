\documentclass[a4paper,12pt,openany]{report}

\usepackage{titling}
\usepackage[margin=1in,headsep=0.5in,footskip=0.5in]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{url}

\setlength{\parindent}{12pt}

\newcommand{\subtitle}[1]{
	\posttitle{
		\par\end{center}
		\begin{center}\large#1\end{center}
		\vskip0.5em
	}
}

\newcommand{\coverphoto}[2]{
	\postdate{
		\par\end{center}
		\begin{center}
			\includegraphics[width=15cm]{#1}
		\end{center}
	}
}

\begin{document}
% title page
\title{CITS3401 Data Exploration and Mining\\
Project 2}
\subtitle{Wine Classification}
\author{Mitchell Pomery\\
21130887}
%\coverphoto{images/constructive}{XKCD 810}
\maketitle

\clearpage

% document

\section*{Introduction}
\paragraph \indent
The project specified that we are to develop several classifiers for wines using different classification methods to compare how machine learning performs compared to experts when rating different wines.
The initial data is split into two groups, red wine and white wine, available from the UCI Machine Learning Repository\cite{datasetlocation}.
Data analysis was done using Weka\cite{weka}, data mining software created by Machine Learning Group at the University of Waikato, New Zealand.
Specifically, we are using the classification tools in Weka Explorer for analysis.

\section*{Data Preprocessing}
\paragraph \indent
The initial data provided was in two files, \texttt{winequality-red.csv} and \texttt{winequality\-white.csv}, that where converted to Weka's ARFF file format using an online conversion tool\cite{csv2arff}.
This tool was used to output two datasets, dataset 1 (\texttt{ds1-red.arff} and \texttt{ds1-white.arff}) and dataset 2 (\texttt{ds2-red.arff} and \texttt{ds2-white.arff}).
Dataset 1 contains all the information that was in the original data, and is used to create the classifier.
The fields in this dataset are numeric, apart from the quality which is nominal, making it is possible to group wines that receive the same rankings in Weka.
Dataset 2 is contains all the numerical information from the original data and does not contain any information about the rankings from the wine tasters.
The aim is to cluster these so that the wines fall into groups similar to the quality attribute of dataset 1, and then create classifiers for this dataset to rate the wine.

\section*{Clustering}
\paragraph \indent
Dataset 2 requires clustering before it can be classified as the quality attribute of each data point has been removed.


\section*{Classification}

\subsection*{Naive Bayesian}
\paragraph \indent
Naive Bayes classifiers are simple to implement, fast, and are used in real world situations such as spam filters.
They work by looking at the traits of an object, and using each individual trait to determine how likely it is that the object falls into a specific classification.
Their downside is that they assume the presence or absence of particular traits has no affect on the classification.

\paragraph \indent
In Weka, we used 

\subsubsection*{Dataset 1}
\paragraph \indent

\subsubsection*{Dataset 2}
\paragraph \indent

\subsection*{Support Vector Machine}
\paragraph \indent
Support Vector Machine's are learning models and algorithms that analyze data and find patterns, then use the patterns for classification of data.
Unlike the Naive Bayesian classifier, the support vector machine is a non-probabilistic classifier, meaning that it will not provide uncertainty for the results.
This means that each different category is separated by as large a gap as possible.

\subsubsection*{Dataset 1}
\paragraph \indent

\subsubsection*{Dataset 2}
\paragraph \indent

\subsection*{Neural Network}
\paragraph \indent
Neural Networks are based off of animal's central nervous systems, by using several input sensors that transform the data before handing it on to another neuron.
The neurons are connected together in a network and work simultaneously , rather then sequentially, to process the data.
Real world applications for neural networks include speech and handwriting recognition.

\subsubsection*{Dataset 1}
\paragraph \indent

\subsubsection*{Dataset 2}
\paragraph \indent

\section*{Results}
\paragraph \indent

\begin{thebibliography}{10}
	\bibitem{datasetlocation}UCI Machine Learning Repository: Wine Quality Data Set. 2014. UCI Machine Learning Repository: Wine Quality Data Set. [ONLINE] Available at: \url{http://archive.ics.uci.edu/ml/datasets/Wine+Quality}. [Accessed 01 June 2014].

	\bibitem{weka}Weka 3 - Data Mining with Open Source Machine Learning Software in Java . 2014. Weka 3 - Data Mining with Open Source Machine Learning Software in Java . [ONLINE] Available at: \url{http://www.cs.waikato.ac.nz/ml/weka/}. [Accessed 01 June 2014].

	\bibitem{csv2arff}Online CSV to ARFF conversion tool. 2014. Online CSV to ARFF conversion tool. [ONLINE] Available at: \url{http://slavnik.fe.uni-lj.si/markot/csv2arff/csv2arff.php}. [Accessed 01 June 2014].

	\bibitem{priorpaper}P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.

	\bibitem{naivebayes}Rennie, J.; Shih, L.; Teevan, J.; Karger, D. (2003). "Tackling the poor assumptions of Naive Bayes classifiers". ICML.

\bibitem{asd}Non-Probabilistic Classiﬁcation Methods. 2014. Non-Probabilistic Classiﬁcation Methods. [ONLINE] Available at: \url{http://www.dcs.gla.ac.uk/~girolami/Machine_Learning_Module_2006/week_5/Lectures/wk_5.pdf}. [Accessed 01 June 2014].

\end{thebibliography}

\end{document}